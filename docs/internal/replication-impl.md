# レプリケーション機能の実装方針

## 本ドキュメントについて

本ドキュメントは、以下のドキュメントを元に、レプリケーションの最小構成について
実装方針を記述したものです。本ドキュメントでは、「レプリケーションの最小構成」を単に「最小構成」と記述することが
あります。

* [レプリケーションの最小構成](https://nautilus-technologies.app.box.com/file/1794091239958)
* [レプリケーション最小構成の通信プロトコル](https://nautilus-technologies.app.box.com/file/1798577910013)


## 基本方針


### マスター

Limestone以外は、基本的に既存のTsurugiのモジュールをそのまま使用し、Limestone単独でレプリケーションに対応し、マスターとして動作する。

* 短期間で最小構成のレプリケーションを実現するために、Limestone以外のモジュールとの依存を避けたいのが最大の理由
* このため、本来は別の方法をとるべきところで暫定的な対応をすることがある。
  * 例えば、レプリカとの通信パラメータを、tsurugi.iniでなく環境変数などで取得するなど。


### レプリカ

limestoneの独立コマンドとして実装する。

* 既存のDatastore, 通信関連のクラス、mainメソッドを持ち、前述の二つのモジュールをコントロールするモジュールから構成する。

* レプリケーションの最小構成では終了手段を用意しない。終了したいときは、プロセスを終了させるシグナルを送信して終了する。

* 最小構成ではレプリカは1台のみ


### プロトコル

* 最初の実装では、プロトコル定義を完全に満たす実装は目指さない
  * 例えば、マスターは、`LOG_OPERATION_SESSION_END`,  `LOG_OPERATION_FLUSH`の両方のフラグを
    立てたメッセージしか送らないので、レプリカはどちらか片方のフラグのみ立っているケースに対する
    ロジックを実装しない。
  * 最小構成の動作に必要な通信のみをプロトコル定義に従って実装する


## 処理フロー

### マスター

#### マスター:初期化

* `datastore::ready`の呼び出しまでは、通常のシーケンスで起動。
* `datastore::ready`の処理
  * 環境変数などを経由して最小構成の設定パラメータを読み、通常(レプリケーションなし)で起動するのか、レプリケーションのマスターとして起動するのか決定する。以下、マスターとして起動する場合の処理フロー。
  * レプリカに接続し、コントロールチャネルを作成する。
    * 失敗したらAbortする。
  * ローカルのログチャネルと同じ数だけ、レプリカログチャネルを作成する。
    * 一つでも失敗したら、Abortする。
    * log_channelに、レプリカログチャネルに関する情報を保持可能にする。
      * 保持するのは、ソケットディスクリプタを含む、レプリカログチャネルを含むオブジェクト
    * `std::vector<std::unique_ptr<log_channel>> datastore::log_channels_`の全要素に対してレプリカログチャネルを作成し、セットする。

#### マスター：ログエントリの書き込み

ここでは、各log_channelにおける、`log_channel::begin_session`から`log_channel::end_session`までの処理について述べる。

* `log_channel::begin_session`
  * ローカルストレージへの書き込み後に、レプリカログチャネルにフラグ`LOG_OPERATION_SESSION_BEGIN`を立てた、エントリが空のログエントリを書き込む
  * Ackの応答は待たない(レプリカ側はこのメッセージに対してAckを送信しない)
* `log_channel::add_entry`
  * ローカルストレージへのエントリ書き込み後、レプリカログチャネルにログエントリを書き込む
  * BLOBエントリでは、BLOBファイルもエントリに書き込む
  * Ackの応答は待たない(レプリカ側はこのメッセージに対してAckを送信しない)
  * add_entry以外の以下の呼び出しも同様に処理する
    * remove_entry
    * add_storage
    * remove_storage
    * truncate_storage
* `log_channel::end_session`
  * ローカルストレージへのfsyncと並行して、レプリカログチャネルにフラグ`LOG_OPERATION_SESSION_END`, `LOG_OPERATION_FLUSH` を立てた、エントリが空のログエントリを書き込む。
  * ローカルストレージのfsyncが終了し、かつ`end_session`のACKが返るまでブロックする。
  * この処理でスレッドをたてる必要はなく、fsync前にログエントリを書き込み、fsync完了後に、Ackを待つのでもよい。


#### マスター:グループコミット

現状のLimestoneは、グループコミットが完了の条件が整うと、次のコールバックを
順次呼び出す。

* `write_epoch_callback`
  * エポックファイルの更新を行うコールバック
* `persistent_callback`
  * shirakamiに永続化完了を通知するコールバック

レプリケーションの最小構成では、`write_epoch_callback`の中で、
epochファイルの更新と共に、コントロールチャネルでグループコミットの通知を行う。
epochファイルの更新とグループコミットのAck受信が、両方とも
終了したら、コールバックを終了する。

その後、`persistent_callback`が呼ばれ、`STORED`と、`PROPAGATED`が
両方満たされたときに、`persistent_callback`が呼ばれる。

**Note**

 エポックファイルの更新と、レプリカのグループコミットの順序は未検討だが、どの順序でも大きな違いはなさそう

1. エポックファイルの更新 => レプリカへのグループコミット指示 => Ack受信
2. レプリカへのグループコミット指示 => Ack受信 => エポックファイルの更新
3. レプリカへのグループコミット指示 => エポックファイルの更新 => Ack受信 



### レプリカ


#### レプリカ:初期化

* mainメソッドの処理
  * datastoreの初期化 => `datastore::ready`を呼び出しまで実行。
    * 初期化のために、ログディレクトリの情報が必要 
  * ソケットをリッスンする。
  * ソケットをアクセプトする。
    * 最初にアクセプトとするのは、レプリカ・コントロールチャネル
      * 先に、レプリカ・ログチャネルの要求が来たらAbort
      * レプリカ・コントロールチャネル作成後、さらにレプリカ・コントロールチャネル作成の要求が来たらAbort
      * レプリカ・ログチャネルの作成が終了したら、
    * レプリカ・ログチャネルをアクセプトしたら、`datastore::create_channel`でローカルのログチャネルを呼び出す。
    * アクセプトしたら、当該チャネル用のスレッドを割り当て、当該チャネルの処理はそのスレッドが受け持つ
  * マスター側からは、すべてのログチャネルの作成が完了したかについての情報は伝わらないが特に問題ない。


#### レプリカ：ログ書き込み

* レプリカは、レプリカ・ログチャネルから受け取ったログエントリを、そのままローカルのログチャネルに書き出す。
  * 基本的に受信したエントリに合致するlog_channelのメソッドを呼び出す。
  * 対応するエントリ
    * begin_session
    * add_entry
    * remove_entry
    * add_storage
    * remove_storage
    * truncate_storage
* フラグ`LOG_OPERATION_SESSION_BEGIN`が立っている場合、受信したログエントリの書き込みに先立ちmarker_beginのエントリを書き込む。
* フラグ`LOG_OPERATION_SESSION_END`、| `LOG_OPERATION_FLUSH` のどちらかが立っているログエントリを受信した場合、すべてのエントリの書き込み後に、`log_channel::end_session`を呼び出し、ログチャネルのフラッシュとSYNCを行う。完了したら、ACKを返す。

#### レプリカ：グループコミット

* ~~マスターからグループコミットのメッセージを受信したら、switch_epochを呼び出す。~~
* ~~グループコミット完了の条件が整うと、write_epoch_callbackが呼ばれるので、そこでEpochの書き込みを行い、さらにコントロールチャネルに対してグループコミットのACKを送信する。~~

マスターからグループコミットのEpcoh IDを送ってもうまくいかない、送るべきなのは、グループコミットのキッカケとなったswitch_epochのEpoch IDである。
といっても、この値を得るのは難しいので、レプリカではswitch_epochを呼び出さず、直接epochファイルを更新し、更新が終了したらAckを返すことにする。

これで正しく動くためには、マスタ側で`log_channel::end_session`のAckを待つ必要がある。現在、Ackを待つ実装なので
問題ないが、将来的に、Ackを待たない実装にする場合は、注意が必要。


## persistent_callback

LimestoneからShirakamiへの永続化の通知は、既存のI/Fをそのまま使用する。既存では以下のどちらかの条件が満たされたときに、`persistent_callback`が呼ばれる。ただし複数のEpochがほぼ同時に条件を満たした場合は、古いEpochに対する通知が省略されることがある。

1. 特定のEpochのローカルストレージへの永続化が完了したとき(`STORED`)。
2. 特定のEpochで永続化が行われないことが確定したとき。 

レプリケーションの最小構成では、1.の通知をローカルストレージへの永続化完了(`STORED`)とレプリカへの伝搬が両方とも完了したとき(`PROPAGATED`)に
persistent_callbackが呼ばれるようになる。

## その他諸々

### 既存のDatastoreのI/Fは変更しない。

この現行のI/Fを維持する

1. `log_channel`を通してWALを書き込む
2. 同一の`log_channel`インスタンスのメソッドが同時に呼ばれることはない
3. `log_channel::begin_session`で特定Epochのセッションを開始し、`log_channel::end_session`で、セッションを終了しローカルディスクの永続化を行う。
4. `log_channel::begin_session`と、`log_channel_end_session`のあいだに`log_channel::add_entry`などのメソッドを使用してWALを書き込む。
5. `data_store::switch_epoch`でエポックの切り替えを行う。エポックの切り替えと、`log_channel`によるWALの永続化は非同期に行われる。また、異なる`log_channel`間では呼び出しは同期されず任意のタイミングで呼び出される。
6. `data_store::switch_epoch`でエポックの切り替えが起きた後、最新のEpochより古いEpochで、すべてのセッションが終了していて、かつ当該セッションより、古いすべてのEpochのセッションが終了しているときに、永続化が完了したとみなし、`persistent_callback`を呼び出す。



### レプリカへのメッセージ送信とAckの受信について

#### `log_channel::begin_session`および`log_channel::add_entry`等

* 同一スレッドで、ローカルディスクへの書き込み、レプリカへの送信を順次行う
  * データが大きくない限り、どちらもOSのバッファへの書き込みであり、短時間に終了することが予想される。
  * ローカルディスクへの書き込みはバッファキャッシュへの書き込み、レプリカへの送信はソケットバッファへの書き込みであり、前者の方が圧倒的にサイズが大きく長時間ブロックされる可能性が低いので、先に実行する。
  * 別スレッドで並列で書き込む案もある。
    * 書き込みサイズが小さい場合、スレッドの生成/Wakeupなどのコストが高く並列化してもさほど効果がないと考えられる(ただし、結論は測定してだすべき)
    * 最小構成では実装量を減らしたい。
* `TCP_NODELAY`を無効にしてソケットに書き込み、小さなパケットが大量に送信されるのを抑制する。

#### `log_channel::end_session`

* 現実装では、`log_channel::end_session`ローカルディスクへの書き込みは行わず、当該ログチャネルのファイルストリームに対して、fsyncを実行する。
* 最小構成では、これに加えてレプリカに対して、メッセージを送信し、Ackを待つ。
  * `TCP_NODELAY`を有効にして送信し、レプリカにすぐにメッセージが届くようにする。
  * レプリカは、当該セッションのすべての書き込みが完了し、さらにfsyncを実行、fsync完了後にAckを返す。
  * `begin_session`および`add_entry`等のメッセージは、同一のソケットで、`end_session`に先だって送信されるので、レプリカは、`end_session`時に同一セッションのすべてのメッセージを受信済みであることが保証される。
* 上記二つの操作は別スレッドで並列に行う。

**Note**

レプリカは、`log_channel::end_session`では、fsyncの実行後に完了を待たずにackを返し、エポックの切り替え時にfsyncの完了を待つという案もある。
  * こちらの方が、性能面で有利に思えるが、大きな差はないとも思える。
  * 現状のlimestoneのローカルストレージへの永続化が、`log_channel::end_session`でfsyncを行っているのでそれに合わせるなら、この方法は不採用。
  * そもそも、こちらの方が実装が複雑なので、最小構成では不採用。

### etc.

* 一つの`log_channel`は、一つのレプリカに対して、1本のソケットを使用しつづける。これによりメッセージの追い越しがないことが補償される。
* 最小構成では、障害等により、ソケットのセッションが切れることを想定しない。したがって、再接続の方法などについても、議論しない。



### レプリカの動作の補足

最小構成では、その他の機能に対して次のように対応する。

* コンパクション
  * マスターのコンパクションはレプリカに伝搬しない
  * レプリカ単独でのコンパクションは可能
    * オンライン、オフラインともに動作可能    
* BLOBファイルのGC
  * 起動時のGCは動作する。
  * 最小構成ではコンパクション時のGCは動作させない。
    * 既存コードだとコンパクション時のGCが動作するので、動作しないように修正する。
* レプリカに終了機能を用意しない。
  * 必要なら、CTRL+Cで停止する。
  * マスターは、レプリカとのソケットの接続が切断されたことを検知したらAbortする。
* セーフスナップショット切り替え
  * 実装しない(現行のlimestoneに実装がない)

### IPv6対応

* 最小構成ではIPv6に対応しない

## 設定パラメータ

### マスター

* TSURUGI_REPLICATION_ENDPOINT
  * `tcp://<host>:<port>`
  * 接続するレプリカのIPアドレスとポートを指定する

### レプリカ

* TSURUGI_REPLICATION_ENDPOINT
  * `tcp://<host>:<port>`
  * 自身がリッスンするホストとポートを指定する。
* ログディレクトリのパス


## チューニングパラメータ

ここでは、最小構成用に追加するパラメータについて記述する。既存パラメータについては言及しない。
パラメータファイル等で変更可能にすることを想定しているが、定数で保持するだけかもしれない。

* ソケットバッファのサイズ



## configuration_id

### 基本方針

* `configuration_id`とはレプリカが正しいマスタと接続しているかを確認するために使用するID
* レプリケーション機能の(利用)有無にかかわらず、データストアの初期化時にUUIDを生成する。
  このUUIDを、`instance_uuid`と呼ぶ。
* `instance_uuid`は`limestone-manifest.json`に記録する。
* 現状の、`limestone-manifest.json`は以下の通り(固定値)
    ```json
    {
        "format_version": "1.0",
        "persistent_format_version": 3
    }
    ```  
* これを以下のように変更する。
  * "format_version"は、`limestone-manifest.json`のフォーマットバージョンを表す。これを"1.0"から"1.1"に変更する。
  * 新たに項目、"instance_uuid"を追加する。
  * "instance_uuid"はRFC 4122 バージョン4の 36 文字表記（16進数32桁＋ハイフン4個）とする。
    * `boost::uuids::to_string`で出力される文字列を使用する。
  * 例
  ```json
  {
      "format_version": "1.1",
      "persistent_format_version": 3,
      "instance_uuid": "550e8400-e29b-41d4-a716-446655440000"
  } 
  ```

* **configuration_id** は、`limestone-manifest.json` を
  **正規化した JSON 文字列そのもの** を指す。
  - 構成要素: `format_version`, `persistent_format_version`, `instance_uuid`
  - これらのいずれかが異なれば別インスタンスとみなす。
  - 将来、`limestone-manifest.json` に新たな項目を追加する場合は、その項目を
    `configuration_id` に含める必要があるかを個別に検討する。
  - 正規化の方法については実装時に決定するものとしここでは議論しない。  

* マイグレーション方針
  * 起動時に、"format_version": "1.0"であれば、無条件に"format_version": "1.1"に変更し、"instance_uuid"を追加する。
  * `limestone-manifest.json`を安全に更新するための仕組みは、"persistent_format_version"のマイグレーション用に作成したロジックをそのまま使用する。

* バックアップ、リストア
  * バックアップ、リストア対象の変更はないので、特に考慮しなくてもよい。
  * リストア時には`limestone-manifest.json`のマイグレーションは行わない。リストア後の最初の起動で必要なら`limestone-manifest.json`のマイグレーションを行われる。

* レプリカの起動時に、`limestone-manifest.json`のマイグレーションを行う。
* マスタからレプリカに接続する場合、レプリカはマスタから送信されたconfiguration_idが、自らのconfiguration_idと一致するかを確認する。一致しない場合、接続に失敗する。
* レプリカがデータファイルをもたない場合は、configuration_idのチェックのうち、`instance_uuid`のチェックを行わない。
* "format_version", "persistent_format_version"は想定通りかをチェックを行う。
* サーバから送られた`instance_uuid`を用いて `limestone-manifest.json`を生成する。


### 実装方針

既存コードを次のように修正する。

* 新規に`limestone-manifest.json`を作成する場合は、上記のように生成する。
* 初期化時に、`limestone-manifest.json`を読み、"persistent_format_version"のチェックを行っている処理に次の処理を追加する。
  * "format_version": "1.0",の場合、"format_version": "1.1"に変更し、"instance_uuid"を追加する。
* configuration_idは`datastore`クラスのフィールドとして保持する。
  * 具体的な内容は実装時に決定する。ここでは議論しない。


#### テスト項目

UT作成時にテストを漏らさないためのメモ。この他にもテストが必要なので、
これだけテストすればよいという意味ではない。


- v1.0 manifest で起動 → v1.1 + instance_uuid が自動生成される
- マスタ v1.1 とレプリカ v1.0 → レプリカ側で自動マイグレーション後に接続成功
- configuration_id 不一致 → 接続拒否 (ログに理由)
- レプリカ無ファイル → 初回接続で manifest が生成され、再起動不要