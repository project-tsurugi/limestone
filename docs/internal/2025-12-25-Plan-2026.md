## 概要

* [dist-tx-benchmark](https://github.com/project-tsurugi/dist-tx-benchmark)を、[検証実行環境v0.pdf](https://nautilus-technologies.app.box.com/file/2086268702327)のC1, C2の構成で動作させ、性能を測る。
* 3DBでなく2DBでやる
  * テーブル3つなので理想は3DBだがサーバの台数などの都合があるので
* 正常系のみ実装し、測定する。
  * rollback/rewindは実現していなくてOK

## アーキテクチャ => TPモニタ

### TPモニタの考え方

* 2 個以上のトランザクションが参加する
* 1 個以上のバリアが存在する
  * 各トランザクションは、特定のバリアに到達したことを TPモニタに通知する
  * TPモニタは、すべての参加トランザクションが特定のバリアに到達 (成功して) いることを確認したら、バリアを通過する許可を各トランザクションに与える
  * TPモニタは、いずれかの参加トランザクションが特定のバリアに到達できなかった (失敗した) ら、すべてのトランザクション参加者に失敗を通知し、各バリアは通過できない

### バリア位置の候補

* (1) プレコミット完了直前
  * shirakami に介入し、プレコミットが完了する直前に TPモニタへバリア到達の通知を行う
  * Pros.
    * インデックスの書き込み前に分散トランザクションの失敗を検出できるため、インデックスの巻き戻しが発生しない
  * Cons.
    * Shirakami 内部に作りこむ必要がある
* (2) グループコミット周辺
  * limestone に介入し、グループコミットの開始前、または完了直前に TPモニタへバリア到達の通知を行う
  * Pros.
    * 処理の大半を limestone とその周辺の改造で済ませられる
  * Cons.
    * プレコミットを通過してしまっているため、当該バリアでエラーとなった場合にはインデックスの巻き戻しが発生する可能性がある

### 処理の流れ

以下は、(2) の方針で検討する。

#### 参加者

* AP1: アプリケーション1 (TPモニタ作成者)
  * Tsurugi1でトランザクションを実行
* AP2: アプリケーション2 (TPモニタ参加者)
  * Tsurugi2でトランザクションを実行

#### 正常系

* (1) TPモニタ初期化
  * (1-1) AP1 は、TPモニタのインスタンスを生成する（今回の実装ではパラメータは指定せず、参加者数=2を前提とする）
  * (1-2) AP1 は、取得した TpmID を AP2 に通知する
* (2) AP1 処理
  * (2-1) AP1 は、Tsurugi 上でトランザクションを開始する
  * (2-2) AP1 は、開始したトランザクションから TxID (トランザクション識別子) を取得する
  * (2-3) AP1 は、TPモニタのインスタンスに対し、参加者として TxID を登録する
  * (2-4) AP1 は、limestone に対し、TPモニタのインスタンスと TxID を通知する
  * (2-5) AP1 は、トランザクション処理を実行し、コミットを要求する
  * (2-6) AP1 は、トランザクションのコミット完了を受け取ったら、TPモニタを破棄する
* (3) AP2 処理
  * AP2 は、(2) と同様の処理を実行する
* (4) 各 Tsurugi 内部処理 (2-5, 3-5 の裏側)
  * (4-1) shirakami は、トランザクションのプレコミットを実施する
  * (4-2) shirakami は、limestone に対しトランザクションの永続化を要求する (このとき、TxID を limestone に引き渡す)
  * (4-3) limestone は、TxID に関連する TPモニタを探し、存在すればバリア到達の通知を行う
  * (4-4) limestone は、TPモニタからバリア通過の許可を受け取るまで待機する
  * (4-5) limestone は、バリアを通過したら、トランザクションの永続化を実施する
* (5) TPモニタ処理 (4-4 の裏側)
  * (5-1) TPモニタは、すべての参加者からバリア到達の通知を受け取るまで待機する
  * (5-2) TPモニタは、すべての参加者から通知を受け取ったら、各参加者にバリア通過の許可を通知する

#### 異常系

実験的システムのため、異常系はクライアントでハンドルすることにする。
この場合、クライアントが即死した際に耐えられないので、実際にはあまりよくない。

* (2-5) でエラーが返された場合
  * (2-5-a1) AP1 は、TPモニタに対し、処理の失敗を通知する
  * (2-5-a2) TPモニタは、バリアで待ち合わせるすべての参加者に失敗を通知する
  * (2-5-a3) AP2 は、TPモニタからの失敗通知を受け取り、トランザクションを失敗させる

### 別案(※1)

* 方針
  * プレコミット完了直前にバリアを配置し、かつ limestone 側で TPモニタに参加するというハイブリッド案
* デザイン
  * limestone は、`limestone::datastore::check_commit(TxID)` という関数を提供する
    * Shirakami は、トランザクションのプレコミット完了直前に `check_commit(TxID)` を呼び出す
    * `check_commit(TxID)` の内部では、TPモニタに対し、バリア参加の処理を行う
    * ただし、 `check_commit(TxID)` は長期間ブロックしてしまう可能性があるので、 `try_check_commit(TxID)` のような非ブロッキング版も必要に応じて提供する
    * `TxID` が TPモニタと関連付けられていなければ、特に何も行わない
  * TxID とバリアを関連付ける処理は、上記の案と同様にアプリケーションと limestone 間で行う

## 実装方針(案)

* 正常系のみ動作し、dist-tx-benchmarkが動作することを目的とする。
  * まず正常系が動作する最低限の実装を行う
* 異常系は動作しなくても良い。
  * タイムアウト処理を作り込まない。
  * トランザクション失敗時のAbort, Rewind処理を作り込まない。
  * TPモニタのキャンセル処理を作り込まない。
* AP1, AP2は同一プロセス
  * [検証実行環境v0.pdf](https://nautilus-technologies.app.box.com/file/2086268702327)のC1, C2の構成では、AP1, AP2を別サーバ別プロセスで実行。
  * プロセス間通信の実装が必要なり開発量が増えるので、別サーバ別プロセスであることが重要でなければ同一プロセスとする。
* バリアをどこに置くのかは要検討だが、実装方針により限定される。
  * 最低限の実装では、TX内にバリアは一箇所のみ
  * トランザクションの過程で複数のバリアが存在するが、最低限の実装では、その中の一つのバリアを実装する。また、TPモニタの仕様もバリアが一つであることを前提としている。
* ここでの記述では ※1 の別案は考慮していない。別案に関しては以下のように考えている。
  * limestone側の対応は容易
  * shirakami側の対応詳細は要検討

### TPモニタ

* TPモニタのためのgRPCサービスを実装する。
* gRPCサーバをどこに置くのか(どこのサーバ)は、別途検討。
  * 独立プロセス
  * Tsurugiインスタンス1 or Tsurugiインスタンス2
  * AP1, AP2と同一プロセス
* 提供するサービスメソッド
  * 1: TPモニタ作成
    * パラメータ:
      * なし（参加者数は2固定）
    * 戻り値: TpmID(TPモニタ識別子)
  * 2: TPモニタ参加: 既存のTPモニタにトランザクションを参加させる
    * パラメータ:
      * TpmID
      * TxID
      * TsID
    * 戻り値: 成功/失敗
  * 3: 1,2 をまとめて行うメソッド
    * パラメータ:
      * TxID1, TsID1
      * TxID2, TsID2
    * 戻り値: TpmID
    * 備考: 最低限の実装の場合、1, 2なしで3だけで良いかも。
  * 4: TPモニタ破棄
    * パラメータ: TpmID
    * 戻り値: 成功/失敗
    * 備考:最低限の実装の場合なくてもよい
      * すべての参加者がバリアに到達したら自動的に破棄
      * 異常系を作り込まないなら問題ない
  * 5: バリア到達通知
    * パラメータ:
      * TpmID
      * TxID
    * 戻り値: 成功/失敗
    * 備考
      * すべての参加者から通知が来るまでブロックする。
      * 異常系は考慮しないので、タイムアウトなどは実装しない。

※ すべてローカルPCで動かすならロックファイルを使った実装や、IPCを使った実装なども
   考えられるが、Tsurugi Server1, 2, クライアント(AP1, AP2)を別サーバで動かすことも
  考慮し、実装が容易と思われるgRPCで実装する。

### dist-tx-benchmark

* 現状では、1トランザクションで3つのテーブルにアクセスしているのを、2つのトランザクションに分割する
  * AP1, AP2
* 先にTPモニタを作成してTpmIDを共有し、各トランザクション開始後に参加させる
* TsurugiにTxIDとTpmIDを通知する。
  * このためのI/Fをtsubakuroに追加する。
* この後、SQL実行 => コミットの流れはそのまま  
* AP1, AP2は、自分がトランザクションを実行するTsurugi ServerのTsIDを知っている必要がある。
  * 設定ファイル、環境変数などで指定する。
* AP1, AP2は、同一プロセス内でよいか？
  * 別プロセスを別サーバで動かす必要があるか  

### tsubakuro

* クライアントからTxIDとTpmIDを受け取り、limestoneに通知するI/Fを追加する。

### shirakami

* limestoneのbegin_session呼び出し時に、TxIDを通知する。
  * limestoneは、パラメータTxIDをもつbegin_sessionを提供する。

### limestone

* tsubakuroから、TxID, TpmIDを受け付けるI/Fを追加する。
  * 受け取ったTxID, TpmIDを、map<TxID, TpmID>の形で保持する。

* begin_sessionに、TxIDパラメータを追加する.
  * 受け取ったTxIDに対応するTpmIDをmapから取得する。
    * mapに存在しない場合のエラー処理 => 最低限の実装ではログに出力するのみ
  * このときのEpochを、記録する。
    * map<Epoch, List<TxID>>の形で保持する。
* グループコミット時
  * map<Epoch, List<TxID>>から、当該Epochに対応するTxIDリストを取得する。
  * 各TxIDに対応するTpmIDをmap<TxID, TpmID>から取得する。
  * TPモニタに対して、各TpmIDのTPモニタにバリア到達通知を行う。
    * TpmID, TxIDをパラメータとして渡す。
  * TPモニタは、すべての参加者から通知が来るまでブロックする。
  * ブロック解除されたら、グループコミットを続行する。
    * 最低限の実装ではタイムアウト処理を行わない。
* limestoneは、自分のTsIDを知っている必要がある。
  * 設定ファイル、環境変数などで指定する。

※ このデータの持ち方だとmutexが必要となりmutexで詰まる原因となるなど、改善の余地がある。実装時に見直すが最低限の実装ではこのままかもしれない。

## 測定項目

* いくつかの構成で、スループット(TPM)と遅延を測定する。
* 以下の構成から選択
    * A1: 単一DB、分散TXなし、レプリカなし
    * B1-0: 単一DB、分散TXなし、同一サーバ内にレプリカ
    * B1-1: 単一DB、分散TXなし、別サーバにレプリカ
    * C1-0: 2DB、分散TXあり、同一サーバ内にレプリカ
    * C1-2: 2DB、分散TXあり、別サーバにレプリカ
* 遅延
  * 以下の遅延のうちいくつかを計測
  * クライアントサイドで計測可能なもの
    * TX開始〜コミットまで(クライアントサイド)
  * Tsurugi 1, 2サイドで計測可能なもの
    * グループコミット開始〜バリア到達まで()
    * TPモニタまち時間
    * バリア通過からグループコミット完了まで
  * 他にあるか？
    * 容易に測定可能という観点で
    * 実験として必要という観点で
